{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68303a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AZCourtOrdersScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://orders.azcourts.gov\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        self.all_orders = []\n",
    "    \n",
    "    def get_years_to_scrape(self):\n",
    "        \"\"\"Generate list of years from 1956 to current year\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        return list(range(1956, current_year + 1))\n",
    "    \n",
    "    def construct_year_url(self, year):\n",
    "        \"\"\"Construct URL for a specific year's administrative orders\"\"\"\n",
    "        # Two URL patterns based on year:\n",
    "        # 2015 and before: AdministrativeOrdersIndex/2015AdministrativeOrders.aspx\n",
    "        # 2016 and after: Administrative-Orders-Index/2016-Administrative-Orders\n",
    "        \n",
    "        if year <= 2015:\n",
    "            # Old .aspx format (2015 and before)\n",
    "            return f\"{self.base_url}/AdministrativeOrdersIndex/{year}AdministrativeOrders.aspx\"\n",
    "        else:\n",
    "            # New dash format (2016 and after)\n",
    "            return f\"{self.base_url}/Administrative-Orders-Index/{year}-Administrative-Orders\"\n",
    "    \n",
    "    def try_url_patterns(self, year):\n",
    "        \"\"\"Get the URL for the year and try to access it\"\"\"\n",
    "        url = self.construct_year_url(year)\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                format_type = \".aspx format\" if year <= 2015 else \"dash format\"\n",
    "                print(f\"DEBUG: Year {year} - SUCCESS ({format_type}): {url}\")\n",
    "                return response, url\n",
    "            else:\n",
    "                print(f\"DEBUG: Year {year} - HTTP {response.status_code}: {url}\")\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"DEBUG: Year {year} - ERROR: {str(e)[:50]}... URL: {url}\")\n",
    "            return None, None\n",
    "    \n",
    "    def scrape_year_page(self, year):\n",
    "        \"\"\"Scrape administrative orders for a specific year\"\"\"\n",
    "        \n",
    "        # Try both URL patterns\n",
    "        response, working_url = self.try_url_patterns(year)\n",
    "        \n",
    "        if not response or not working_url:\n",
    "            print(f\"ERROR: No working URL found for year {year}\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            orders = []\n",
    "            \n",
    "            # DEBUG: Print the page structure to understand what we're parsing\n",
    "            print(f\"\\nDEBUG: Successfully accessing {year} - {working_url}\")\n",
    "            \n",
    "            # Look for different table patterns\n",
    "            tables = soup.find_all('table')\n",
    "            print(f\"DEBUG: Found {len(tables)} tables\")\n",
    "            \n",
    "            # Also look for other common containers\n",
    "            divs_with_tables = soup.find_all('div', class_=lambda x: x and 'table' in x.lower() if x else False)\n",
    "            print(f\"DEBUG: Found {len(divs_with_tables)} divs with table-related classes\")\n",
    "            \n",
    "            # Look for any structure that might contain orders\n",
    "            all_links = soup.find_all('a', href=True)\n",
    "            pdf_links = [link for link in all_links if '.pdf' in link.get('href', '').lower()]\n",
    "            print(f\"DEBUG: Found {len(pdf_links)} PDF links on page\")\n",
    "            \n",
    "            for table_idx, table in enumerate(tables):\n",
    "                print(f\"DEBUG: Processing table {table_idx + 1}\")\n",
    "                rows = table.find_all('tr')\n",
    "                print(f\"DEBUG: Table has {len(rows)} rows\")\n",
    "                \n",
    "                # Skip if not enough rows\n",
    "                if len(rows) < 2:\n",
    "                    continue\n",
    "                \n",
    "                # Check the first few rows to understand structure\n",
    "                for row_idx, row in enumerate(rows[:3]):\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    cell_texts = [cell.get_text().strip()[:50] for cell in cells]  # Limit length for debug\n",
    "                    print(f\"DEBUG: Row {row_idx}: {cell_texts}\")\n",
    "                \n",
    "                # Look for the correct table structure\n",
    "                # We expect: Order Number | Description | Date\n",
    "                header_row = rows[0] if rows else None\n",
    "                if header_row:\n",
    "                    header_cells = header_row.find_all(['th', 'td'])\n",
    "                    header_texts = [cell.get_text().strip().lower() for cell in header_cells]\n",
    "                    print(f\"DEBUG: Header row: {header_texts}\")\n",
    "                    \n",
    "                    # Check if this looks like an orders table\n",
    "                    has_order_header = any('order' in h or 'no.' in h for h in header_texts)\n",
    "                    has_date_header = any('date' in h or 'signed' in h for h in header_texts)\n",
    "                    has_description_header = any('description' in h or 'title' in h or 'subject' in h for h in header_texts)\n",
    "                    \n",
    "                    print(f\"DEBUG: Table analysis - Order: {has_order_header}, Date: {has_date_header}, Description: {has_description_header}\")\n",
    "                    \n",
    "                    # Be more lenient - if it has at least 3 columns, try to process it\n",
    "                    if len(header_cells) < 3:\n",
    "                        print(f\"DEBUG: Skipping table {table_idx + 1} - less than 3 columns\")\n",
    "                        continue\n",
    "                \n",
    "                # Process data rows (skip header)\n",
    "                data_rows = rows[1:] if len(rows) > 1 else []\n",
    "                \n",
    "                if not data_rows:\n",
    "                    print(f\"DEBUG: No data rows in table {table_idx + 1}\")\n",
    "                    continue\n",
    "                \n",
    "                with tqdm(total=len(data_rows), \n",
    "                         desc=f\"Year {year}\", \n",
    "                         leave=False, \n",
    "                         unit=\"rows\",\n",
    "                         ncols=80,\n",
    "                         ascii=True,\n",
    "                         disable=False) as pbar:\n",
    "                    \n",
    "                    for row in data_rows:\n",
    "                        cells = row.find_all(['td', 'th'])\n",
    "                        \n",
    "                        if len(cells) >= 3:  # Should have at least 3 columns\n",
    "                            try:\n",
    "                                # Extract order number and link\n",
    "                                order_cell = cells[0]\n",
    "                                order_link = order_cell.find('a')\n",
    "                                \n",
    "                                if order_link:\n",
    "                                    order_number = order_link.text.strip()\n",
    "                                    href = order_link.get('href', '')\n",
    "                                    pdf_link = urljoin(self.base_url, href) if href else \"\"\n",
    "                                else:\n",
    "                                    order_number = order_cell.get_text().strip()\n",
    "                                    pdf_link = \"\"\n",
    "                                \n",
    "                                # Extract description (try different columns if needed)\n",
    "                                description = cells[1].get_text().strip()\n",
    "                                \n",
    "                                # Extract date (might be in different column)\n",
    "                                date_signed = \"\"\n",
    "                                for i in range(2, len(cells)):\n",
    "                                    cell_text = cells[i].get_text().strip()\n",
    "                                    # Look for date patterns\n",
    "                                    if re.search(r'\\d{1,2}[/\\-]\\d{1,2}[/\\-]\\d{2,4}', cell_text):\n",
    "                                        date_signed = cell_text\n",
    "                                        break\n",
    "                                \n",
    "                                # If no date found in expected places, use last column\n",
    "                                if not date_signed and len(cells) > 2:\n",
    "                                    date_signed = cells[-1].get_text().strip()\n",
    "                                \n",
    "                                print(f\"DEBUG: Raw data - Order: '{order_number}', Desc: '{description[:30]}...', Date: '{date_signed}'\")\n",
    "                                \n",
    "                                # VALIDATION: More flexible validation\n",
    "                                # Skip obviously bad entries\n",
    "                                if not order_number or len(order_number) > 50:\n",
    "                                    print(f\"DEBUG: Skipping - invalid order number length\")\n",
    "                                    pbar.update(1)\n",
    "                                    continue\n",
    "                                \n",
    "                                if not description or len(description) < 5:\n",
    "                                    print(f\"DEBUG: Skipping - description too short\")\n",
    "                                    pbar.update(1)\n",
    "                                    continue\n",
    "                                \n",
    "                                # Clean up the data\n",
    "                                order_number = re.sub(r'\\s+', ' ', order_number)\n",
    "                                description = re.sub(r'\\s+', ' ', description)\n",
    "                                date_signed = re.sub(r'\\s+', ' ', date_signed)\n",
    "                                \n",
    "                                order_data = {\n",
    "                                    'Order_Number': order_number,\n",
    "                                    'Administrative_Order_Description': description,\n",
    "                                    'Date_Signed': date_signed,\n",
    "                                    'Link_Order': pdf_link,\n",
    "                                    'Year': year\n",
    "                                }\n",
    "                                orders.append(order_data)\n",
    "                                print(f\"DEBUG: âœ“ Added order: {order_number}\")\n",
    "                            \n",
    "                            except Exception as e:\n",
    "                                print(f\"DEBUG: Error processing row in {year}: {e}\")\n",
    "                                continue\n",
    "                        \n",
    "                        # Update progress bar\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix({'Found': len(orders)})\n",
    "            \n",
    "            print(f\"DEBUG: Year {year} complete. Found {len(orders)} valid orders\")\n",
    "            return orders\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Unexpected error for {year}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_all_years(self, start_year=1956, end_year=None):\n",
    "        \"\"\"Scrape all years from start_year to end_year (or current year)\"\"\"\n",
    "        if end_year is None:\n",
    "            end_year = datetime.now().year\n",
    "            \n",
    "        years = list(range(start_year, end_year + 1))\n",
    "        total_years = len(years)\n",
    "        \n",
    "        print(f\"Starting scrape of {total_years} years from {start_year} to {end_year}\")\n",
    "        print(\"Years with no orders will be skipped automatically.\")\n",
    "        \n",
    "        # Force flush to ensure print appears\n",
    "        import sys\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Main progress bar for years with better settings\n",
    "        with tqdm(total=total_years, \n",
    "                 desc=\"Overall Progress\", \n",
    "                 unit=\"year\",\n",
    "                 ncols=100,\n",
    "                 ascii=True,\n",
    "                 position=0,\n",
    "                 leave=True) as main_pbar:\n",
    "            \n",
    "            for i, year in enumerate(years):\n",
    "                main_pbar.set_description(f\"Scraping {year}\")\n",
    "                orders = self.scrape_year_page(year)\n",
    "                \n",
    "                # Only add orders if we found any\n",
    "                if orders:\n",
    "                    self.all_orders.extend(orders)\n",
    "                    print(f\"Year {year}: Found {len(orders)} orders\")\n",
    "                else:\n",
    "                    print(f\"Year {year}: No orders found - skipping\")\n",
    "                \n",
    "                # Update main progress bar\n",
    "                main_pbar.update(1)\n",
    "                main_pbar.set_postfix({\n",
    "                    'Year': year,\n",
    "                    'This_Year': len(orders) if orders else 0,\n",
    "                    'Total_Orders': len(self.all_orders),\n",
    "                    'Completed': f\"{i+1}/{total_years}\"\n",
    "                })\n",
    "                \n",
    "                # Print periodic updates for debugging\n",
    "                if (i + 1) % 5 == 0 or i == 0:\n",
    "                    print(f\"Completed {i+1}/{total_years} years. Total orders: {len(self.all_orders)}\")\n",
    "                    sys.stdout.flush()\n",
    "                \n",
    "                # Be respectful with requests\n",
    "                time.sleep(1)\n",
    "        \n",
    "        print(f\"\\nScraping complete! Total orders scraped: {len(self.all_orders)}\")\n",
    "        return self.all_orders\n",
    "    \n",
    "    def save_to_excel(self, filename=\"az_court_administrative_orders.xlsx\"):\n",
    "        \"\"\"Save scraped data to Excel file\"\"\"\n",
    "        if not self.all_orders:\n",
    "            print(\"No data to save. Run scrape_all_years() first.\")\n",
    "            return\n",
    "        \n",
    "        # Progress bar for data processing\n",
    "        with tqdm(total=4, desc=\"Processing data\") as pbar:\n",
    "            pbar.set_description(\"Creating DataFrame\")\n",
    "            df = pd.DataFrame(self.all_orders)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            pbar.set_description(\"Reordering columns\")\n",
    "            # Reorder columns to match requested format\n",
    "            column_order = ['Order_Number', 'Administrative_Order_Description', 'Date_Signed', 'Link_Order']\n",
    "            df = df[column_order + [col for col in df.columns if col not in column_order]]\n",
    "            pbar.update(1)\n",
    "            \n",
    "            pbar.set_description(\"Sorting data\")\n",
    "            # Sort by year and order number\n",
    "            df = df.sort_values(['Year', 'Order_Number']) if 'Year' in df.columns else df.sort_values('Order_Number')\n",
    "            pbar.update(1)\n",
    "            \n",
    "            pbar.set_description(\"Saving to Excel\")\n",
    "            # Save to Excel\n",
    "            df.to_excel(filename, index=False, engine='openpyxl')\n",
    "            pbar.update(1)\n",
    "        \n",
    "        print(f\"Data saved to {filename}\")\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Print sample of data\n",
    "        print(\"\\nSample data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        return df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the scraper\"\"\"\n",
    "    print(\"Arizona Court Administrative Orders Scraper\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    scraper = AZCourtOrdersScraper()\n",
    "    \n",
    "    # Full historical scrape from 1956 to present\n",
    "    current_year = datetime.now().year\n",
    "    total_years = current_year - 1956 + 1\n",
    "    \n",
    "    print(f\"Starting FULL historical scrape from 1956 to {current_year}...\")\n",
    "    print(f\"Checking {total_years} years. Years with no orders will be skipped.\")\n",
    "    print(\"This may take 30-60 minutes depending on your connection speed.\")\n",
    "    \n",
    "    print(f\"\\nðŸš€ Starting scrape of {total_years} years...\")\n",
    "    print(\"Progress bars and updates will appear below.\")\n",
    "    \n",
    "    # Full historical scrape - starts from 1956\n",
    "    orders = scraper.scrape_all_years(1956)\n",
    "    \n",
    "    if orders:\n",
    "        print(f\"\\nâœ… Scraping completed! Found {len(orders)} total orders.\")\n",
    "        df = scraper.save_to_excel(\"az_court_orders_complete_1956_2025.xlsx\")\n",
    "        \n",
    "        # Show comprehensive statistics\n",
    "        if not df.empty:\n",
    "            print(f\"\\nðŸ“Š FINAL STATISTICS:\")\n",
    "            print(f\"   Years covered: {df['Year'].min()} - {df['Year'].max()}\")\n",
    "            print(f\"   Total orders: {len(df):,}\")\n",
    "            print(f\"   Years with data: {len(df['Year'].unique())}\")\n",
    "            \n",
    "            print(f\"\\nðŸ“ˆ Orders by decade:\")\n",
    "            df['Decade'] = (df['Year'] // 10) * 10\n",
    "            decade_counts = df['Decade'].value_counts().sort_index()\n",
    "            for decade, count in decade_counts.items():\n",
    "                print(f\"   {decade}s: {count:,} orders\")\n",
    "            \n",
    "            print(f\"\\nðŸ“… Most recent years:\")\n",
    "            recent_years = df['Year'].value_counts().sort_index().tail(10)\n",
    "            for year, count in recent_years.items():\n",
    "                print(f\"   {year}: {count} orders\")\n",
    "                \n",
    "            print(f\"\\nðŸ’¾ Data saved to: az_court_orders_complete_1956_2025.xlsx\")\n",
    "    else:\n",
    "        print(\"âŒ No orders were scraped. Please check the URLs and HTML structure.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Additional utility functions for more targeted scraping:\n",
    "\n",
    "def scrape_single_year(year):\n",
    "    \"\"\"Scrape orders for a single year\"\"\"\n",
    "    scraper = AZCourtOrdersScraper()\n",
    "    orders = scraper.scrape_year_page(year)\n",
    "    \n",
    "    if orders:\n",
    "        scraper.all_orders = orders\n",
    "        df = scraper.save_to_excel(f\"az_court_orders_{year}.xlsx\")\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "def scrape_full_range():\n",
    "    \"\"\"Scrape the complete range from 1956 to present\"\"\"\n",
    "    print(\"WARNING: This will scrape 69+ years of data and may take considerable time.\")\n",
    "    response = input(\"Continue? (y/n): \")\n",
    "    \n",
    "    if response.lower() == 'y':\n",
    "        scraper = AZCourtOrdersScraper()\n",
    "        print(\"Starting full historical scrape...\")\n",
    "        orders = scraper.scrape_all_years(1956)\n",
    "        \n",
    "        if orders:\n",
    "            df = scraper.save_to_excel(\"az_court_orders_complete.xlsx\")\n",
    "            return df\n",
    "    return None\n",
    "\n",
    "# Test tqdm functionality\n",
    "def test_tqdm():\n",
    "    \"\"\"Test if tqdm is working properly in your environment\"\"\"\n",
    "    print(\"Testing tqdm functionality...\")\n",
    "    import sys\n",
    "    \n",
    "    # Test 1: Simple progress bar\n",
    "    print(\"Test 1: Basic tqdm functionality\")\n",
    "    for i in tqdm(range(10), desc=\"Basic test\", ascii=True, ncols=80):\n",
    "        time.sleep(0.1)\n",
    "    print(\"âœ“ Basic tqdm test completed\")\n",
    "    \n",
    "    # Test 2: Check environment\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"Running in: {'Jupyter' if 'ipykernel' in sys.modules else 'Terminal'}\")\n",
    "    \n",
    "    # Test 3: Different tqdm settings\n",
    "    print(\"Test 2: tqdm with different settings\")\n",
    "    with tqdm(total=5, desc=\"Settings test\", unit=\"item\", ascii=True) as pbar:\n",
    "        for i in range(5):\n",
    "            time.sleep(0.2)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'Item': i+1})\n",
    "    print(\"âœ“ Settings test completed\")\n",
    "    \n",
    "    print(\"All tqdm tests passed! Progress bars should work.\")\n",
    "\n",
    "# Quick test function for the scraper\n",
    "def quick_test():\n",
    "    \"\"\"Test scraper with just one recent year\"\"\"\n",
    "    print(\"Quick test: Scraping just 2025...\")\n",
    "    scraper = AZCourtOrdersScraper()\n",
    "    orders = scraper.scrape_year_page(2025)\n",
    "    \n",
    "    if orders:\n",
    "        print(f\"âœ“ Successfully found {len(orders)} orders for 2025\")\n",
    "        print(\"Sample order:\", orders[0] if orders else \"None\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âœ— No orders found for 2025\")\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
