{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685bda5e",
   "metadata": {},
   "source": [
    "# Azure OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab812f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing blackbooks: 100%|██████████| 16/16 [00:01<00:00, 12.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 2490 entries to ../files/excel/all_blackbooks_orders.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# debug_blackbook_markdown_dump.py\n",
    "import os, base64, sys, time, traceback\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import DocumentContentFormat\n",
    "\n",
    "IN_DIR  = Path(\"files/blackbooks\")        # adjust if needed\n",
    "OUT_DIR = Path(\"files/blackbooks_md\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    load_dotenv()\n",
    "    AZURE_KEY = os.getenv(\"AZURE_KEY\")\n",
    "    AZURE_ENDPOINT = os.getenv(\"AZURE_ENDPOINT\")\n",
    "\n",
    "    print(f\"CWD: {Path.cwd()}\")\n",
    "    print(f\"IN_DIR exists: {IN_DIR.exists()}  -> {IN_DIR.resolve()}\")\n",
    "    print(f\"OUT_DIR: {OUT_DIR.resolve()}\")\n",
    "    print(f\"AZURE_ENDPOINT set: {bool(AZURE_ENDPOINT)}  AZURE_KEY set: {bool(AZURE_KEY)}\")\n",
    "    if not AZURE_KEY or not AZURE_ENDPOINT:\n",
    "        print(\"❌ Missing AZURE_KEY or AZURE_ENDPOINT in environment. Check your .env.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    ai_client = DocumentIntelligenceClient(\n",
    "        endpoint=AZURE_ENDPOINT,\n",
    "        credential=AzureKeyCredential(AZURE_KEY)\n",
    "    )\n",
    "\n",
    "    files = sorted(list(IN_DIR.rglob(\"*.pdf\")) + list(IN_DIR.rglob(\"*.docx\")))\n",
    "    print(f\"Found {len(files)} file(s):\")\n",
    "    for p in files[:10]:\n",
    "        print(\"  -\", p)\n",
    "    if len(files) > 10:\n",
    "        print(f\"  ... and {len(files)-10} more\")\n",
    "\n",
    "    if not files:\n",
    "        print(\"⚠️ No .pdf or .docx files found. Check the path and extensions.\")\n",
    "        print(\"Tip: Are you running this from the repo root? Try absolute paths.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    processed = 0\n",
    "    for i, path in enumerate(files, 1):\n",
    "        print(f\"[{i}/{len(files)}] {path} …\", end=\"\", flush=True)\n",
    "        try:\n",
    "            data = path.read_bytes()\n",
    "            poller = ai_client.begin_analyze_document(\n",
    "                model_id=\"prebuilt-layout\",\n",
    "                body={\"base64Source\": base64.b64encode(data).decode()},\n",
    "                output_content_format=DocumentContentFormat.MARKDOWN,\n",
    "            )\n",
    "            res = poller.result()\n",
    "\n",
    "            # Prefer page-based extraction if pages are populated\n",
    "            md_text = \"\"\n",
    "            if getattr(res, \"pages\", None):\n",
    "                parts = []\n",
    "                for page in res.pages:\n",
    "                    if not page.spans:\n",
    "                        continue\n",
    "                    span = page.spans[0]\n",
    "                    parts.append(res.content[span.offset : span.offset + span.length])\n",
    "                md_text = \"\\n\\n---\\n\\n\".join(parts)\n",
    "            else:\n",
    "                md_text = res.content or \"\"\n",
    "\n",
    "            if not md_text.strip():\n",
    "                print(\" empty markdown returned\")\n",
    "            else:\n",
    "                out_file = OUT_DIR / f\"{path.stem}.md\"\n",
    "                out_file.write_text(md_text, encoding=\"utf-8\")\n",
    "                print(f\" saved -> {out_file.name}\")\n",
    "                processed += 1\n",
    "        except Exception as e:\n",
    "            print(\" FAILED\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"✅ Done. Processed {processed}/{len(files)} file(s) in {dt:.2f}s.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17fd2bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading markdowns: 100%|██████████| 16/16 [00:00<00:00, 2164.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 493,116 chars\n",
      "smart_chunk_text: produced 85 chunks (max_chars=6000, overlap=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting bodies: 100%|██████████| 85/85 [04:22<00:00,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2193 bodies of rules (pre-dedup)\n",
      "Deduped: 2193 → 2090\n",
      "✅ Excel saved to /Users/othmanbensouda/Desktop/jobtalk_paper/excel/extracted_blackbooks_bodies_llm.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Blackbook Extraction — ONE ROW PER *NAMED + DATED* BODY OF RULES (LLM)\n",
    "- Counts \"(2) AMENDED ORDER AMENDING RULES …\"\n",
    "- Skips headings containing \"CORRECTIVE\"\n",
    "- Overlap=0, anchor_excerpt fingerprint, strong dedup\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- LLM ----\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ====== Schemas ======\n",
    "class RuleBody(BaseModel):\n",
    "    body_title: str = Field(description=\"Exact name of the body of rules (e.g., 'Rules of Civil Procedure', 'Local Rules of Practice for the Yuma County Superior Court')\")\n",
    "    parent_order_title: str = Field(description=\"The full heading/title block of the order this body came from (everything before the first Dated/Filed/Effective/Approved line)\")\n",
    "    issued_year: int = Field(description=\"Earliest year from date lines inside the order (Dated/Filed/Effective/Approved). If none present, use nearest year header above the order.\")\n",
    "    anchor_excerpt: str = Field(description=\"First ~120 chars of parent_order_title (before date lines), whitespace-collapsed\")\n",
    "    category: str = Field(description=\"Short family inferred from the body title, e.g., 'Rules of the Supreme Court', 'Local Rules of Practice', 'Uniform Rules of Practice', 'Rules of Criminal Procedure'. If unclear, return empty string.\")\n",
    "\n",
    "class BodiesList(BaseModel):\n",
    "    bodies: List[RuleBody]\n",
    "\n",
    "# ====== Chunking ======\n",
    "# Capture likely starts (ORDER..., any heading containing RULES, plus common ‘ADOPTION OF …’ / ‘STATE BAR – …’ banners)\n",
    "UNICODE_DASH = r\"\\u2010-\\u2015\\u2212\\uFE58\\uFE63\\uFF0D\\-–—\"\n",
    "PUNCT = r\" .,:;/'\\\"()\\[\\]&/\"\n",
    "\n",
    "ORDER_START_REGEX = re.compile(\n",
    "    rf\"\"\"(?mix)\n",
    "    ^\\s*\n",
    "    (?:\\(\\d+\\)\\s*)?                                     # optional numbering e.g. (2)\n",
    "    (?:\n",
    "        (?:AMENDED\\s+)?ORDER\\b[^\\n]*                    # ORDER... or AMENDED ORDER...\n",
    "        |\n",
    "        [A-Z{PUNCT}{UNICODE_DASH}]*\\bRULES\\b[^\\n]*      # any heading line containing RULES\n",
    "        |\n",
    "        ADOPTION\\s+OF\\b[^\\n]*                           # ADOPTION OF ...\n",
    "        |\n",
    "        STATE\\s+BAR\\b[^\\n]*                             # STATE BAR — RULES OF THE SUPREME COURT etc.\n",
    "    )\\s*$\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "YEAR_HEADER_REGEX = re.compile(r'(?m)^\\s*((?:19|20)\\d{2})\\s*$')\n",
    "\n",
    "def smart_chunk_text(text: str, max_chars: int = 6000, overlap: int = 0) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split on likely order starts and year headers, pack segments into chunks <= max_chars.\n",
    "    Overlap kept at 0 to avoid cross-chunk duplicates.\n",
    "    \"\"\"\n",
    "    t = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    starts = [m.start() for m in ORDER_START_REGEX.finditer(t)]\n",
    "    starts += [m.start() for m in YEAR_HEADER_REGEX.finditer(t)]\n",
    "    starts = sorted(set([0] + starts + [len(t)]))\n",
    "\n",
    "    segments = []\n",
    "    for i in range(len(starts) - 1):\n",
    "        seg = t[starts[i]:starts[i+1]]\n",
    "        if seg.strip():\n",
    "            segments.append(seg)\n",
    "\n",
    "    chunks, cur = [], \"\"\n",
    "    for seg in segments:\n",
    "        if not cur:\n",
    "            cur = seg\n",
    "            continue\n",
    "        if len(cur) + len(seg) <= max_chars:\n",
    "            cur += seg\n",
    "        else:\n",
    "            chunks.append(cur.strip())\n",
    "            cur = seg  # overlap=0\n",
    "    if cur.strip():\n",
    "        chunks.append(cur.strip())\n",
    "\n",
    "    chunks = [c for c in chunks if len(c.strip()) > 50]\n",
    "    print(f\"smart_chunk_text: produced {len(chunks)} chunks (max_chars={max_chars}, overlap={overlap})\")\n",
    "    return chunks\n",
    "\n",
    "# ====== LLM extraction per chunk ======\n",
    "LLM_MODEL = \"gpt-4.1\"  # or \"gpt-4o-mini\" if you want cheaper/faster\n",
    "\n",
    "def build_prompt(chunk: str, chunk_id: int) -> str:\n",
    "    return f\"\"\"\n",
    "You are extracting **bodies of rules** from Blackbook text. A *valid row* is anything that is BOTH:\n",
    "(1) **Named** (a heading like \"ORDER …\", \"STATE BAR – RULES OF THE SUPREME COURT:\", \"LOCAL RULES …\", \"UNIFORM RULES …\", \"ADOPTION OF …\", etc.)\n",
    "AND\n",
    "(2) **Dated** (has at least one date line such as \"Dated …\", \"Filed …\", \"Effective …\", or \"Approved by Supreme Court …\" with a year).\n",
    "\n",
    "Your unit of output is **ONE BODY OF RULES** (not one order).\n",
    "- A single order can contain multiple distinct bodies (e.g., \"Rules of Civil Procedure\" AND \"Rules of Criminal Procedure\"). Output a row for EACH body.\n",
    "- Do **NOT** split when \"and\" only connects rule **numbers** within the SAME body (e.g., \"amending Rules 16, 26, 37\" → one body).\n",
    "- Treat entries like \"(2) AMENDED ORDER AMENDING RULES 26.11, 29, 30, and 41, RULES OF CRIMINAL PROCEDURE [R-18-0028]\" as **valid** if dated.\n",
    "- **Exclude** items whose heading contains the word **CORRECTIVE**.\n",
    "- **parent_order_title** = full heading/title block up to the first date line (include any quote marks, punctuation, brackets, petition IDs, etc.).\n",
    "- **issued_year** = the **earliest** year in the date lines (Dated/Filed/Effective/Approved). If no date line is present, use the nearest year header above the item.\n",
    "- **anchor_excerpt** = the first ~120 characters of parent_order_title (whitespace collapsed).\n",
    "- **category** = concise family label derived from the body title (e.g., \"Local Rules of Practice\", \"Rules of the Supreme Court\",\n",
    "  \"Uniform Rules of Practice\", \"Rules of Criminal Procedure\", \"Rules of Procedure in Traffic Cases\"). If unclear, return \"\".\n",
    "\n",
    "Edge cases that MUST count:\n",
    "1) \"(2) AMENDED ORDER AMENDING RULES 26.11, 29, 30, and 41, RULES OF CRIMINAL PROCEDURE [R-18-0028]\"\n",
    "   → body_title should reflect the body of rules (e.g., \"Rules of Criminal Procedure\"), not the petition number.\n",
    "\n",
    "Return ONLY structured data with fields:\n",
    "- body_title\n",
    "- parent_order_title\n",
    "- issued_year\n",
    "- anchor_excerpt\n",
    "- category\n",
    "\n",
    "Text chunk {chunk_id}:\n",
    "{chunk}\n",
    "\"\"\".strip()\n",
    "\n",
    "def extract_chunk(chunk_data):\n",
    "    chunk, chunk_id = chunk_data\n",
    "    try:\n",
    "        llm = ChatOpenAI(model=LLM_MODEL, temperature=0)\n",
    "        structured_llm = llm.with_structured_output(BodiesList)\n",
    "        result = structured_llm.invoke(build_prompt(chunk, chunk_id))\n",
    "        return [b.model_dump() for b in result.bodies]\n",
    "    except Exception as e:\n",
    "        print(f\"[LLM ERROR] chunk {chunk_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "# ====== Post: normalize + strong dedup ======\n",
    "def normalize_for_dedup(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def norm(s: str) -> str:\n",
    "        s = (s or \"\")\n",
    "        s = s.lower()\n",
    "        s = re.sub(r'\\s+', ' ', s)\n",
    "        s = re.sub(r'[\\u2010-\\u2015\\u2212\\uFE58\\uFE63\\uFF0D\\-–—]+', '-', s)  # unify dashes\n",
    "        return s.strip(' :;,.()[]{}')\n",
    "\n",
    "    for col in [\"body_title\", \"parent_order_title\", \"anchor_excerpt\", \"category\"]:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    df[\"body_title_norm\"] = df[\"body_title\"].map(norm)\n",
    "    df[\"parent_order_title_norm\"] = df[\"parent_order_title\"].map(norm)\n",
    "    df[\"anchor_excerpt_norm\"] = df[\"anchor_excerpt\"].map(norm)\n",
    "    return df\n",
    "\n",
    "def strong_dedup(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(\n",
    "        subset=[\"body_title_norm\", \"parent_order_title_norm\", \"anchor_excerpt_norm\", \"issued_year\"],\n",
    "        keep=\"first\"\n",
    "    ).copy()\n",
    "    after = len(df)\n",
    "    print(f\"Deduped: {before} → {after}\")\n",
    "    return df\n",
    "\n",
    "# ====== Main ======\n",
    "def main():\n",
    "    # Your markdown folder\n",
    "    input_dir = Path(\"/Users/othmanbensouda/Desktop/jobtalk_paper/files/blackbooks_md\")\n",
    "    md_files = sorted(input_dir.glob(\"*.md\"))\n",
    "    if not md_files:\n",
    "        print(f\"[ERR] No markdown files found in {input_dir}\")\n",
    "        return\n",
    "\n",
    "    combined = \"\"\n",
    "    for f in tqdm(md_files, desc=\"Reading markdowns\"):\n",
    "        t = f.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        # keep structure; just remove HTML comments and fix hyphen linewraps\n",
    "        t = t.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "        t = re.sub(r'<!--.*?-->', '', t, flags=re.S)\n",
    "        t = re.sub(r'-\\s*\\n\\s*', '', t)\n",
    "        combined += \"\\n\\n\" + t\n",
    "\n",
    "    print(f\"Total length: {len(combined):,} chars\")\n",
    "\n",
    "    # Chunk & extract via LLM\n",
    "    chunks = smart_chunk_text(combined, max_chars=6000, overlap=0)\n",
    "    all_rows = []\n",
    "    with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "        futures = {ex.submit(extract_chunk, (chunk, i+1)): i+1 for i, chunk in enumerate(chunks)}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Extracting bodies\"):\n",
    "            rows = fut.result() or []\n",
    "            all_rows.extend(rows)\n",
    "\n",
    "    print(f\"Extracted {len(all_rows)} bodies of rules (pre-dedup)\")\n",
    "    if not all_rows:\n",
    "        print(\"[ERR] 0 rows returned. Consider lowering chunk size or inspecting a sample chunk.\")\n",
    "        return\n",
    "\n",
    "    # DataFrame + dedup\n",
    "    df = pd.DataFrame(all_rows, columns=[\"body_title\",\"parent_order_title\",\"issued_year\",\"anchor_excerpt\",\"category\"])\n",
    "    df[\"issued_year\"] = pd.to_numeric(df[\"issued_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = normalize_for_dedup(df)\n",
    "    df = strong_dedup(df)\n",
    "\n",
    "    # Save\n",
    "    out_dir = Path(\"/Users/othmanbensouda/Desktop/jobtalk_paper/excel\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / \"extracted_blackbooks_bodies_llm.xlsx\"\n",
    "    df.drop(columns=[\"body_title_norm\",\"parent_order_title_norm\",\"anchor_excerpt_norm\"], errors=\"ignore\").to_excel(out_path, index=False)\n",
    "    print(f\"✅ Excel saved to {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b1257bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../excel/extracted_blackbooks_bodies_llm_categorized.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "IN_PATH = \"../excel/extracted_blackbooks_bodies_llm.xlsx\"\n",
    "OUT_PATH = \"../excel/extracted_blackbooks_bodies_llm_categorized.xlsx\"\n",
    "\n",
    "# Load\n",
    "df = pd.read_excel(IN_PATH)\n",
    "\n",
    "# ---- config: set the column that holds the rule/body name ----\n",
    "NAME_COL = \"parent_order_title\"  # change if needed\n",
    "\n",
    "# Drop the column\n",
    "if \"anchor_excerpt\" in df.columns:\n",
    "    df = df.drop(columns=[\"anchor_excerpt\"])\n",
    "\n",
    "# Normalized helper\n",
    "name = df[NAME_COL].fillna(\"\").astype(str).str.lower()\n",
    "\n",
    "# 1) Local vs statewide\n",
    "df[\"is_local_rule\"] = name.str.contains(\"local\", na=False).astype(int)\n",
    "df[\"is_statewide_rule\"] = (~name.str.contains(\"local\", na=False)).astype(int)\n",
    "\n",
    "# 2) Statewide trial court rule\n",
    "is_not_supreme = ~name.str.contains(\"supreme court\", na=False)\n",
    "is_appellate = name.str.contains(\"appellate\", na=False)\n",
    "mentions_superior = name.str.contains(\"superior\", na=False)\n",
    "\n",
    "df[\"is_statewide_trial_court_rule\"] = (\n",
    "    (df[\"is_statewide_rule\"] == 1) &\n",
    "    is_not_supreme &\n",
    "    (~is_appellate | mentions_superior)\n",
    ").astype(int)\n",
    "\n",
    "# Save\n",
    "df.to_excel(OUT_PATH, index=False)\n",
    "print(\"Wrote:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9bcc6bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote plots:\n",
      " - ../excel/fig2_statewide_trial_rules_per_year.png \n",
      " - ../excel/fig3_decade_100pct_stacked.png\n",
      "And tables:\n",
      " - ../excel/blackbooks_fig2_fig3_tables.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IN_PATH  = \"../excel/extracted_blackbooks_bodies_llm_categorized.xlsx\"\n",
    "OUT_XLSX = \"../excel/blackbooks_fig2_fig3_tables.xlsx\"\n",
    "FIG2_PNG = \"../excel/fig2_statewide_trial_rules_per_year.png\"\n",
    "FIG3_DECADE_PNG = \"../excel/fig3_decade_100pct_stacked.png\"\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YEAR_COL = \"issued_year\"\n",
    "LOCAL_COL = \"is_local_rule\"\n",
    "STATEWIDE_TRIAL_COL = \"is_statewide_trial_court_rule\"\n",
    "\n",
    "# ---- Load & normalize ----\n",
    "df = pd.read_excel(IN_PATH).copy()\n",
    "df[YEAR_COL] = pd.to_numeric(df[YEAR_COL], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Universe: local OR statewide-trial\n",
    "trial = df[(df[LOCAL_COL] == 1) | (df[STATEWIDE_TRIAL_COL] == 1)].copy()\n",
    "\n",
    "# -------- FIGURE 2: statewide trial-court rules per year --------\n",
    "fig2 = (trial[trial[STATEWIDE_TRIAL_COL] == 1]\n",
    "        .groupby(YEAR_COL, dropna=True)\n",
    "        .size()\n",
    "        .rename(\"statewide_trial_rules\")\n",
    "        .reset_index())\n",
    "\n",
    "if len(fig2):\n",
    "    yr_min, yr_max = int(fig2[YEAR_COL].min()), int(fig2[YEAR_COL].max())\n",
    "    years = pd.DataFrame({YEAR_COL: range(yr_min, yr_max + 1)})\n",
    "    fig2 = years.merge(fig2, on=YEAR_COL, how=\"left\").fillna({\"statewide_trial_rules\": 0})\n",
    "    fig2[\"statewide_trial_rules\"] = fig2[\"statewide_trial_rules\"].astype(int)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(fig2[YEAR_COL], fig2[\"statewide_trial_rules\"], marker=\"o\")\n",
    "plt.title(\"Figure 2. Statewide rule changes issued by the Arizona Supreme Court per year.\\nIt shows a similarly upward trend.\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG2_PNG, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# -------- FIGURE 3: Decade 100% stacked bar (percentages on each color) --------\n",
    "def decade_label(y):\n",
    "    if pd.isna(y): \n",
    "        return pd.NA\n",
    "    y = int(y)\n",
    "    start = (y // 10) * 10\n",
    "    return f\"{start}-{start+9}\"\n",
    "\n",
    "trial[\"decade\"] = trial[YEAR_COL].apply(decade_label)\n",
    "dcounts = (trial\n",
    "           .groupby(\"decade\", dropna=True)\n",
    "           .agg(local_rules=(LOCAL_COL, \"sum\"),\n",
    "                statewide_rules=(STATEWIDE_TRIAL_COL, \"sum\"))\n",
    "           .reset_index())\n",
    "dcounts[\"trial_total\"] = dcounts[\"local_rules\"] + dcounts[\"statewide_rules\"]\n",
    "dcounts = dcounts[dcounts[\"trial_total\"] > 0].copy()\n",
    "dcounts[\"pct_local\"] = dcounts[\"local_rules\"] / dcounts[\"trial_total\"] * 100\n",
    "dcounts[\"pct_statewide\"] = dcounts[\"statewide_rules\"] / dcounts[\"trial_total\"] * 100\n",
    "\n",
    "# Sort decades chronologically\n",
    "try:\n",
    "    dcounts[\"dec_start\"] = dcounts[\"decade\"].str.split(\"-\").str[0].astype(int)\n",
    "    dcounts = dcounts.sort_values(\"dec_start\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars_local = plt.bar(dcounts[\"decade\"], dcounts[\"pct_local\"], label=\"Local\")\n",
    "bars_state = plt.bar(dcounts[\"decade\"], dcounts[\"pct_statewide\"], \n",
    "                     bottom=dcounts[\"pct_local\"], label=\"Statewide\")\n",
    "\n",
    "# --- Add percentage labels inside each segment ---\n",
    "for bar, pct in zip(bars_local, dcounts[\"pct_local\"]):\n",
    "    if pct > 0:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
    "                 f\"{pct:.0f}%\", ha=\"center\", va=\"center\", fontsize=9, color=\"white\")\n",
    "\n",
    "for bar, bottom, pct in zip(bars_state, dcounts[\"pct_local\"], dcounts[\"pct_statewide\"]):\n",
    "    if pct > 0:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bottom + bar.get_height()/2,\n",
    "                 f\"{pct:.0f}%\", ha=\"center\", va=\"center\", fontsize=9, color=\"white\")\n",
    "\n",
    "plt.title(\"The Share of Trial-Court Rules by Decade (Statewide on Top)\")\n",
    "plt.xlabel(\"Decade\")\n",
    "plt.ylabel(\"Share of Trial-Court Rules (%)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG3_DECADE_PNG, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ---- Save tables ----\n",
    "with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as xw:\n",
    "    fig2.to_excel(xw, index=False, sheet_name=\"fig2_year_counts\")\n",
    "    dcounts.to_excel(xw, index=False, sheet_name=\"fig3_decade_counts_share\")\n",
    "\n",
    "print(\"Wrote plots:\\n -\", FIG2_PNG, \"\\n -\", FIG3_DECADE_PNG)\n",
    "print(\"And tables:\\n -\", OUT_XLSX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd9c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
